# 基础理论学习指南

## 学习目标
本模块旨在帮助学习者建立扎实的深度学习、Transformer和NLP基础知识，为后续LLM的学习打下坚实基础。

## 学习路径

### 1. 深度学习基础

#### 1.1 神经网络原理
- **理论学习**
  - 人工神经元模型
  - 激活函数（ReLU、Sigmoid、Tanh等）
  - 前馈神经网络结构
  - 损失函数选择

- **实践任务**
  - 使用PyTorch实现简单的多层感知机
  - 手写数字识别实验（MNIST数据集）
  - 可视化神经网络的训练过程

#### 1.2 反向传播
- **理论学习**
  - 计算图与自动微分
  - 梯度下降原理
  - 链式法则
  - 反向传播算法推导

- **实践任务**
  - 手动实现反向传播算法
  - 使用自动微分工具（如PyTorch的autograd）
  - 分析不同场景下的梯度流动

#### 1.3 优化算法
- **理论学习**
  - 随机梯度下降（SGD）
  - 动量法（Momentum）
  - Adam优化器
  - 学习率调度策略

- **实践任务**
  - 比较不同优化器的性能
  - 实现学习率衰减策略
  - 可视化优化过程

### 2. Transformer架构

#### 2.1 自注意力机制
- **理论学习**
  - 注意力机制的直观理解
  - Query、Key、Value概念
  - 点积注意力计算
  - 注意力权重的可视化

- **实践任务**
  - 实现基础的自注意力层
  - 序列数据的注意力分析
  - 注意力图的可视化

#### 2.2 多头注意力
- **理论学习**
  - 多头注意力的优势
  - 并行计算原理
  - 头的数量选择

- **实践任务**
  - 实现多头注意力机制
  - 分析不同头的关注点
  - 对比单头和多头性能

#### 2.3 位置编码
- **理论学习**
  - 位置信息的重要性
  - 正弦位置编码原理
  - 可学习位置编码

- **实践任务**
  - 实现正弦位置编码
  - 可视化位置编码矩阵
  - 比较不同位置编码方法

### 3. 自然语言处理基础

#### 3.1 分词技术
- **理论学习**
  - 词的概念与边界
  - BPE分词算法
  - WordPiece和SentencePiece
  - 中文分词特点

- **实践任务**
  - 实现BPE分词算法
  - 使用主流分词工具
  - 评估不同分词方法

#### 3.2 词向量
- **理论学习**
  - One-hot编码
  - Word2Vec原理
  - GloVe模型
  - 上下文词向量

- **实践任务**
  - 训练Word2Vec模型
  - 词向量可视化
  - 词相似度分析

#### 3.3 语言模型基础
- **理论学习**
  - N-gram模型
  - 神经语言模型
  - 困惑度评估
  - 上下文表示

- **实践任务**
  - 实现简单的语言模型
  - 计算模型困惑度
  - 文本生成实验

## 学习资源
- 深度学习：《动手学深度学习》、《深度学习》（花书）
- Transformer：《Attention is All You Need》原论文
- NLP：《自然语言处理综论》、Stanford CS224N课程

## 评估方式
1. 理论知识测验
2. 编程实践作业
3. 项目实现
4. 论文阅读报告

## 预期产出
1. 掌握深度学习基础原理
2. 理解并实现Transformer核心组件
3. 具备基础的NLP技能
4. 完成相关实践项目

## 注意事项
1. 理论与实践结合，每个概念都要动手实现
2. 注重基础原理的理解，不要急于追求高级应用
3. 保持学习记录，记录疑问和解决方案
4. 多参与开源社区，与他人交流学习心得