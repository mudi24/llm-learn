# 语言模型发展历程

## 为什么 ChatGPT 3.5 会在2022年11月底发布后爆火？

从技术的角度有一下三点原因：

1. 模型能力的跃升
ChatGPT基于GPT-3.5架构，引入了**强化学习与人类反馈（RLHF）**技术，显著提升了对话连贯性、逻辑性和安全性。相比GPT-3，它更擅长处理多轮对话、修正错误回答，并减少生成有害内容的能力。
1. 用户交互体验优化
通过对话式界面，ChatGPT降低了AI技术的使用门槛。用户无需编程知识即可直接体验，满足了普通人对AI的直观认知——"像人类一样聊天"。
1. 免费开放测试策略
OpenAI在2022年11月30日向公众开放免费试用，允许用户零成本体验强大的生成能力，迅速积累大规模用户基础。

![llm_history](./images/llm_history.png.png)


## 传统语言模型

### 统计语言模型

#### 链式法则

链式法则，也就是条件概率的乘积。比如，一个句子由多个词组成，每个词的概率依赖于其前面所有词的历史。

```
P(w₁, w₂, ..., wₙ) = P(w₁) · P(w₂|w₁) · P(w₃|w₁, w₂) ... P(wₙ|w₁, ..., wₙ₋₁)
```

缺点：直接计算长序列的复杂度极高。

#### 马尔可夫假设与n-gram模型

为简化计算，引入马尔可夫假设，限定当前词仅依赖前k个词（通常k=1或2），形成n-gram模型：
- **Bigram**（二元模型）：`P(wᵢ|wᵢ₋₁)`
- **Trigram**（三元模型）：`P(wᵢ|wᵢ₋₂, wᵢ₋₁)`
参数数量大幅减少，但牺牲了长距离依赖的捕捉能力。

**参数估计与最大似然**：
 通过统计语料库中的词频估计条件概率。例如，Bigram概率：
 ```
 P(wᵢ|wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)
 ```
 依赖大量数据以确保统计显著性。

##### 统计语言模型对数据稀疏性的处理

**平滑技术**：
 解决零概率问题（未登录n-gram）的常见方法包括：
 - **拉普拉斯平滑**：为所有n-gram计数加1。
 - **Good-Turing估计**：重新分配低频事件的概率。
 - **Kneser-Ney平滑**：考虑词的上下文多样性，尤其适用于高阶n-gram。

**对数概率与计算优化**：
 为避免数值下溢，将概率乘积转换为对数概率求和：
 ```
 log P(w₁, ..., wₙ) = Σᵢ₌₁ⁿ log P(wᵢ|wᵢ₋ₖ, ..., wᵢ₋₁)
 ```

统计语言模型以概率论为核心，通过链式分解和马尔可夫假设简化计算，结合统计估计与平滑技术处理数据稀疏性，为自然语言处理提供了基础框架。

### 神经语言模型

神经语言模型（Neural Language Model, NLM）的核心原理是通过神经网络对自然语言的概率分布进行建模，以预测序列中下一个词（或字符）的概率。其核心思想是用神经网络捕捉语言的统计规律和语义特征，从而生成连贯且符合语境的文本。

早期工作 (MLP) : 单词映射到词向量，再由神经网络预测当前时刻词汇

#### 循环神经网络（RNN）

* 循环结构：通过隐藏状态（Hidden State）逐步处理序列，每一步的输出依赖于当前输入和前一步的隐藏状态。
* 时序依赖：严格按时间步顺序处理序列（从左到右或双向），无法并行计算。
* 典型变体：LSTM（长短期记忆网络）、GRU（门控循环单元），通过门控机制缓解梯度消失问题。
  
RNN 更适合短序列、低资源或实时性要求高的任务，但受限于长距离依赖和训练效率。

#### transformer

* 自注意力机制：通过自注意力（Self-Attention）直接计算序列中所有位置之间的关系，无需逐步处理。
* 并行计算：所有位置的输入同时处理，天然支持并行化。
* 位置编码：通过位置嵌入（Positional Encoding）显式引入位置信息，弥补注意力机制对顺序不敏感的缺陷。

Transformer 凭借全局注意力、并行计算和大规模预训练能力，成为当前NLP的主流架构，但需要更多计算资源。

### 预训练语言模型

### 传统语言模型的局限性

我们把统计语言模型、神经语言模型、预训练语言模型统一成为传统语言模型。

1. 缺乏背景知识
2. 任务泛化性较差
3. 复杂推理能力较弱

大语言模型的出现同时解决了上述问题

## 大语言模型

### 特点

* 模型参数大（数十亿、百亿、千亿甚至万亿）
* 用于预训练的数据规模大
* 与传统语言模型相比，需要更复杂、精细的模型训练方法

### 为什么模型需要有很多参数

### 为什么模型需要大规模的数据用来预训练

* 数据规模越大，模型学习到的能力就越强
* 给模型提供大规模数据，会提升模型的泛化能力

> 数据数量、数据质量决定了模型的能力，同样意味着大算力的需求
