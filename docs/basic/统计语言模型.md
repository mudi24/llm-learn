## 统计语言模型（SLM）

链式法则，也就是条件概率的乘积。比如，一个句子由多个词组成，每个词出现的概率可能依赖于前面的词。

统计语言模型的核心原理是通过概率建模来量化自然语言序列的合理性，其关键在于利用统计学方法估计词序列的联合概率。以下是其核心原理的分点总结：

1. **概率链式法则**：
   基于条件概率分解，句子的联合概率被拆分为条件概率的乘积：
   ```
   P(w₁, w₂, ..., wₙ) = P(w₁) · P(w₂|w₁) · P(w₃|w₁, w₂) ... P(wₙ|w₁, ..., wₙ₋₁)
   ```
   每个词的概率依赖于其前面所有词的历史，但直接计算长序列的复杂度极高。

2. **马尔可夫假设与n-gram模型**：
   为简化计算，引入马尔可夫假设，限定当前词仅依赖前k个词（通常k=1或2），形成n-gram模型：
   - **Bigram**（二元模型）：`P(wᵢ|wᵢ₋₁)`
   - **Trigram**（三元模型）：`P(wᵢ|wᵢ₋₂, wᵢ₋₁)`
   参数数量大幅减少，但牺牲了长距离依赖的捕捉能力。

3. **参数估计与最大似然**：
   通过统计语料库中的词频估计条件概率。例如，Bigram概率：
   ```
   P(wᵢ|wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)
   ```
   依赖大量数据以确保统计显著性。

4. **平滑技术**：
   解决零概率问题（未登录n-gram）的常见方法包括：
   - **拉普拉斯平滑**：为所有n-gram计数加1。
   - **Good-Turing估计**：重新分配低频事件的概率。
   - **Kneser-Ney平滑**：考虑词的上下文多样性，尤其适用于高阶n-gram。

5. **对数概率与计算优化**：
   为避免数值下溢，将概率乘积转换为对数概率求和：
   ```
   log P(w₁, ..., wₙ) = Σᵢ₌₁ⁿ log P(wᵢ|wᵢ₋ₖ, ..., wᵢ₋₁)
   ```

6. **应用与演进**：
   传统统计模型（如n-gram）曾是机器翻译、语音识别的基石，但逐渐被神经网络模型（RNN、Transformer）取代，后者通过分布式表示和注意力机制更有效地捕捉长程依赖。然而，统计语言模型的概率思想和简化假设仍是现代模型的理论基础。

**总结**：统计语言模型以概率论为核心，通过链式分解和马尔可夫假设简化计算，结合统计估计与平滑技术处理数据稀疏性，为自然语言处理提供了基础框架。尽管深度学习方法崛起，其核心原理仍深刻影响着语言建模的发展。

